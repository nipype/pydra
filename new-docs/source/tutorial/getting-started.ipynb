{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "The basic runnable component of Pydra is a *task*. Tasks are conceptually similar to\n",
    "functions, in that they take inputs, operate on them and then return results. However,\n",
    "unlike functions, tasks are parameterised before they are executed in a separate step.\n",
    "This enables parameterised tasks to be linked together into workflows that are checked for\n",
    "errors before they are executed, and modular execution workers and environments to specified\n",
    "independently of the task being performed.\n",
    "\n",
    "Tasks can encapsulate Python functions, shell-commands or workflows constructed from\n",
    "task components.\n",
    "\n",
    "## Running your first task\n",
    "\n",
    "Pre-defined task definitions are installed under the `pydra.tasks.*` namespace by separate\n",
    "task packages (e.g. `pydra-fsl`, `pydra-ants`, ...). To use a pre-defined task definition\n",
    "\n",
    "* import the class from the `pydra.tasks.*` package it is in\n",
    "* instantiate it with appropriate parameters\n",
    "* \"call\" resulting object (i.e. `my_task(...)`) to execute it as you would a function \n",
    "\n",
    "To demonstrate with an example of loading a JSON file with the\n",
    "`pydra.tasks.common.LoadJson` task, we first create an example JSON file to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "import json\n",
    "\n",
    "JSON_CONTENTS = {'a': True, 'b': 'two', 'c': 3, 'd': [7, 0.55, 6]}\n",
    "\n",
    "test_dir = Path(mkdtemp())\n",
    "json_file = test_dir / \"test.json\"\n",
    "with open(json_file, \"w\") as f:\n",
    "    json.dump(JSON_CONTENTS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the JSON contents back from the file using the `LoadJson` task definition\n",
    "class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the task definition\n",
    "from pydra.tasks.common import LoadJson\n",
    "\n",
    "# Instantiate the task definition, providing the JSON file we want to load\n",
    "load_json = LoadJson(file=json_file)\n",
    "\n",
    "# Run the task to load the JSON file\n",
    "outputs = load_json()\n",
    "\n",
    "# Access the loaded JSON output contents and check they match original\n",
    "assert outputs.out == JSON_CONTENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access a richer `Result` object you can use a Submitter object to execute the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydra.engine import Submitter\n",
    "\n",
    "with Submitter(plugin='cf', n_procs=1) as submitter:\n",
    "    result = submitter(load_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Result` object contains\n",
    "\n",
    "* `output`: the outputs of the task (if there is only one output it is called `out` by default)\n",
    "* `runtime`: information about the peak memory and CPU usage\n",
    "* `errored`: the error status of the task\n",
    "* `task`: the task object that generated the results\n",
    "* `output_dir`: the output directory the results are stored in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over inputs\n",
    "\n",
    "It is straightforward to apply the same operation over a set of inputs using the `split()`\n",
    "method. For example, if we wanted to re-grid all the NIfTI images stored in a directory,\n",
    "such as the sample ones generated by the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileformats.medimage import Nifti\n",
    "\n",
    "nifti_dir = test_dir / \"nifti\"\n",
    "nifti_dir.mkdir()\n",
    "\n",
    "for i in range(10):\n",
    "    Nifti.sample(nifti_dir, seed=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can by importing the `MrGrid` shell-command task from the `pydra-mrtrix3` package\n",
    "and then splitting over the list of files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydra.tasks.mrtrix3 import MrGrid\n",
    "\n",
    "# Instantiate the task definition, \"splitting\" over all NIfTI files in the test directory\n",
    "mrgrid = MrGrid(voxel=0.5).split(input=nifti_dir.iterdir())\n",
    "\n",
    "# Run the task to resample all NIfTI files\n",
    "outputs = mrgrid()\n",
    "\n",
    "# Print the locations of the output files\n",
    "print(\"\\n\".join(str(p) for p in outputs.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to iterate over inputs in pairs, if for example you wanted to use\n",
    "different voxel sizes for different images, both the list of images and the voxel sizes\n",
    "are passed to the `split()` method and their combination is specified by a tuple \"splitter\"\n",
    "(see [Splitting and combining](../explanation/splitting-combining.html) for more details\n",
    "on splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of voxel sizes to resample the NIfTI files to, must be the same length\n",
    "# as the number of NIfTI files\n",
    "VOXEL_SIZES = [0.5, 0.5, 0.5, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 1.25]\n",
    "\n",
    "mrgrid_varying_vox_sizes = MrGrid().split(\n",
    "    (\"input\", \"voxel\"),\n",
    "    input=nifti_dir.iterdir(),\n",
    "    voxel=VOXEL_SIZES\n",
    ")\n",
    "\n",
    "\n",
    "submitter = Submitter(cache_dir=test_dir / \"cache\")\n",
    "\n",
    "# Run the task to resample all NIfTI files with different voxel sizes\n",
    "with submitter:\n",
    "    result1 = submitter(mrgrid_varying_vox_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache directories\n",
    "\n",
    "When a task runs, a unique hash is generated by the combination of all the inputs to the\n",
    "task and the operation to be performed. This hash is used to name the output directory for\n",
    "the task within the specified cache directory. Therefore, if you use the same cache\n",
    "directory between runs and in a subsequent run the same task is executed with the same\n",
    "inputs then the location of its output directory will also be the same, and the outputs\n",
    "generated by the previous run are reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrgrid_varying_vox_sizes2 = MrGrid().split(\n",
    "    (\"input\", \"voxel\"),\n",
    "    input=nifti_dir.iterdir(),\n",
    "    voxel=VOXEL_SIZES\n",
    ")\n",
    "\n",
    "\n",
    "# Result from previous run is reused as the task and inputs are identical\n",
    "with submitter:\n",
    "    result2 = submitter(mrgrid_varying_vox_sizes2)\n",
    "\n",
    "\n",
    "# Check that the output directory is the same for both runs\n",
    "assert result2.output_dir == result1.output_dir\n",
    "\n",
    "# Change the voxel sizes to resample the NIfTI files to for one of the files\n",
    "mrgrid_varying_vox_sizes2.inputs.voxel[2] = [0.25]\n",
    "\n",
    "# Result from previous run is reused as the task and inputs are identical\n",
    "with submitter:\n",
    "    result3 = submitter(mrgrid_varying_vox_sizes2)\n",
    "\n",
    "# The output directory will be different as the inputs are now different\n",
    "assert result3.output_dir != result1.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for file objects, the contents of the files are used to calculate the hash\n",
    "not their paths. Therefore, when inputting large files there might be some additional\n",
    "overhead on the first run (the file hashes themselves are cached by path and mtime so\n",
    "shouldn't need to be recalculated unless they are modified). However, this makes the\n",
    "hashes invariant to file-system movement. For example, changing the name of one of the\n",
    "files in the nifti directory won't invalidate the hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a NIfTI file within the test directory\n",
    "first_file = next(nifti_dir.iterdir())\n",
    "first_file.rename(first_file.with_name(\"first.nii.gz\"))\n",
    "\n",
    "mrgrid_varying_vox_sizes3 = MrGrid().split(\n",
    "    (\"input\", \"voxel\"),\n",
    "    input=nifti_dir.iterdir(),\n",
    "    voxel=VOXEL_SIZES\n",
    ")\n",
    "\n",
    "# Result from previous run is reused as the task and inputs are identical\n",
    "with submitter:\n",
    "    result4 = submitter(mrgrid_varying_vox_sizes2)\n",
    "\n",
    "# Check that the output directory is the same for both runs\n",
    "assert result4.output_dir == result1.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging failed tasks\n",
    "\n",
    "Work in progress..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
