{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "\n",
    "This tutorial steps through tecnhiques to identify errors and pipeline failures, as well\n",
    "as avoid common pitfalls setting up executing over multiple processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Things to check if Pydra gets stuck\n",
    "\n",
    "I There are a number of common gotchas, related to running multi-process code, that can\n",
    "cause Pydra workflows to get stuck and not execute correctly. If using the concurrent\n",
    "futures worker (e.g. `worker=\"cf\"`), check these issues first before filing a bug report\n",
    "or reaching out for help.\n",
    "\n",
    "### Applying `nest_asyncio` when running within a notebook\n",
    "\n",
    "When using the concurrent futures worker within a Jupyter notebook you need to apply\n",
    "`nest_asyncio` with the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:07.425751Z",
     "iopub.status.busy": "2025-02-06T10:49:07.425576Z",
     "iopub.status.idle": "2025-02-06T10:49:07.429619Z",
     "shell.execute_reply": "2025-02-06T10:49:07.429079Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is needed to run parallel workflows in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enclosing multi-process code within `if __name__ == \"__main__\"`\n",
    "\n",
    "When running multi-process Python code on macOS or Windows, as is the case when the \n",
    "concurrent futures worker is selected (i.e. `worker=\"cf\"`), then scripts that execute\n",
    "the forking code need to be enclosed within an `if __name__ == \"__main__\"` block, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:07.431414Z",
     "iopub.status.busy": "2025-02-06T10:49:07.431239Z",
     "iopub.status.idle": "2025-02-06T10:49:08.424387Z",
     "shell.execute_reply": "2025-02-06T10:49:08.423873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version (0.25) of nipype/pydra is available. You are using 0.25.dev189+ge439cad6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-2' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-4' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n"
     ]
    }
   ],
   "source": [
    "from pydra.tasks.testing import UnsafeDivisionWorkflow\n",
    "from pydra.engine.submitter import Submitter\n",
    "\n",
    "# This workflow will fail because we are trying to divide by 0\n",
    "wf = UnsafeDivisionWorkflow(a=10, b=5, denominator=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Submitter(worker=\"cf\") as sub:\n",
    "        result = sub(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows the secondary processes to import the script without executing it. Without\n",
    "such a block Pydra will lock up and not process the workflow. On Linux this is not an\n",
    "issue due to the way that processes are forked, but is good practice in any case for\n",
    "code portability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stray lockfiles\n",
    "\n",
    "When a Pydra task is executed, a lockfile is generated to signify that the task is running.\n",
    "Other processes will wait for this lock to be released before attempting to access the\n",
    "tasks results. The lockfiles are automatically deleted after a task completes, either\n",
    "successfully or with an error, within a *try/finally* block so should run most of the time.\n",
    "However, if a task/workflow is terminated by an interactive\n",
    "debugger, the finally block may not be executed, leaving stray lockfiles. This\n",
    "can cause the Pydra to hang waiting for the lock to be released. If you suspect this to be\n",
    "an issue, and there are no other jobs running, then simply remove all lock files from your\n",
    "cache directory (e.g. `rm <your-run-cache-dir>/*.lock`) and re-submit your job.\n",
    "\n",
    "If the  `clean_stale_locks` flag is set (by default when using the *debug* worker), locks that\n",
    "were created before the outer task was submitted are removed before the task is run.\n",
    "However, since these locks could be created by separate submission processes, ``clean_stale_locks`\n",
    "is not switched on by default when using production workers (e.g. `cf`, `slurm`, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting errors\n",
    "\n",
    "### Running in *debug* mode\n",
    "\n",
    "By default, Pydra will run with the *debug* worker, which executes each task serially\n",
    "within a single process without use of `async/await` blocks, to allow raised exceptions\n",
    "to propagate gracefully to the calling code. If you are having trouble with a pipeline,\n",
    "ensure that `worker=debug` is passed to the submission/execution call (the default).\n",
    "\n",
    "### Reading error files\n",
    "\n",
    "When a task raises an error, it is captured and saved in pickle file named `_error.pklz`\n",
    "within task's cache directory. For example, when calling the toy `UnsafeDivisionWorkflow`\n",
    "with a `denominator=0`, the task will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:08.426400Z",
     "iopub.status.busy": "2025-02-06T10:49:08.426164Z",
     "iopub.status.idle": "2025-02-06T10:49:09.155363Z",
     "shell.execute_reply": "2025-02-06T10:49:09.154786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-6' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-7' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Divide'>: attribute lookup Divide on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-9' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=ZeroDivisionError('float division by zero')>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 254, in _process_worker\n",
      "    r = call_item.fn(*call_item.args, **call_item.kwargs)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 202, in unpickle_and_run\n",
      "    return task.run(rerun=rerun)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/core.py\", line 368, in run\n",
      "    self.definition._run(self)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/specs.py\", line 633, in _run\n",
      "    returned = self.function(**inputs)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/tasks/testing/__init__.py\", line 11, in Divide\n",
      "    return x / y\n",
      "           ~~^~~\n",
      "ZeroDivisionError: float division by zero\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "ZeroDivisionError: float division by zero\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Task Divide raised an error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m wf \u001b[38;5;241m=\u001b[39m UnsafeDivisionWorkflow(a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(denominator\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m ,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Submitter(worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m sub:\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merrored:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkflow failed with errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(result\u001b[38;5;241m.\u001b[39merrors))\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:194\u001b[0m, in \u001b[0;36mSubmitter.__call__\u001b[0;34m(self, task_def)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task\u001b[38;5;241m.\u001b[39mis_async:  \u001b[38;5;66;03m# Only workflow tasks can be async\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrerun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mrun(task, rerun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrerun)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py:199\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:51\u001b[0m, in \u001b[0;36mWorker.run_async\u001b[0;34m(self, task, rerun)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[DefType]\u001b[39m\u001b[38;5;124m\"\u001b[39m, rerun: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mrun_async(rerun\u001b[38;5;241m=\u001b[39mrerun)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/core.py:422\u001b[0m, in \u001b[0;36mTask.run_async\u001b[0;34m(self, rerun)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudit\u001b[38;5;241m.\u001b[39mmonitor()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39m_run_async(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    423\u001b[0m     result\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39mOutputs\u001b[38;5;241m.\u001b[39m_from_task(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/specs.py:721\u001b[0m, in \u001b[0;36mWorkflowDef._run_async\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[WorkflowDef]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the workflow asynchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39msubmitter\u001b[38;5;241m.\u001b[39mexpand_workflow_async(task)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:342\u001b[0m, in \u001b[0;36mSubmitter.expand_workflow_async\u001b[0;34m(self, workflow_task)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task\u001b[38;5;241m.\u001b[39mis_async:\n\u001b[0;32m--> 342\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mrun_async(task, rerun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrerun)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m         task_futures\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mrun(task, rerun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrerun))\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:51\u001b[0m, in \u001b[0;36mWorker.run_async\u001b[0;34m(self, task, rerun)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[DefType]\u001b[39m\u001b[38;5;124m\"\u001b[39m, rerun: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mrun_async(rerun\u001b[38;5;241m=\u001b[39mrerun)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/core.py:422\u001b[0m, in \u001b[0;36mTask.run_async\u001b[0;34m(self, rerun)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudit\u001b[38;5;241m.\u001b[39mmonitor()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39m_run_async(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    423\u001b[0m     result\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39mOutputs\u001b[38;5;241m.\u001b[39m_from_task(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/specs.py:721\u001b[0m, in \u001b[0;36mWorkflowDef._run_async\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[WorkflowDef]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the workflow asynchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39msubmitter\u001b[38;5;241m.\u001b[39mexpand_workflow_async(task)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:346\u001b[0m, in \u001b[0;36mSubmitter.expand_workflow_async\u001b[0;34m(self, workflow_task)\u001b[0m\n\u001b[1;32m    344\u001b[0m             task_futures\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mrun(task, rerun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrerun))\n\u001b[1;32m    345\u001b[0m     task_futures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mfetch_finished(task_futures)\n\u001b[0;32m--> 346\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_runnable_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m workflow_task\u001b[38;5;241m.\u001b[39mreturn_values \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m: wf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m: exec_graph}\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:393\u001b[0m, in \u001b[0;36mSubmitter.get_runnable_tasks\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    391\u001b[0m node: NodeExecution\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39msorted_nodes:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m:\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# since the list is sorted (breadth-first) we can stop\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# when we find a task that depends on any task that is already in tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:509\u001b[0m, in \u001b[0;36mNodeExecution.done\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarted:\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:521\u001b[0m, in \u001b[0;36mNodeExecution.update_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Check to see if any previously queued tasks have completed\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueued\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m:\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccessful[task\u001b[38;5;241m.\u001b[39mstate_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueued\u001b[38;5;241m.\u001b[39mpop(index)\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m task\u001b[38;5;241m.\u001b[39merrored:\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/core.py:462\u001b[0m, in \u001b[0;36mTask.done\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _result\u001b[38;5;241m.\u001b[39merrored:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m raised an error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Task Divide raised an error"
     ]
    }
   ],
   "source": [
    "# This workflow will fail because we are trying to divide by 0\n",
    "wf = UnsafeDivisionWorkflow(a=10, b=5).split(denominator=[3, 2 ,0])\n",
    "\n",
    "with Submitter(worker=\"cf\") as sub:\n",
    "    result = sub(wf)\n",
    "    \n",
    "if result.errored:\n",
    "    print(\"Workflow failed with errors:\\n\" + str(result.errors))\n",
    "else:\n",
    "    print(\"Workflow completed successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error pickle files can be loaded using the `cloudpickle` library, noting that it is\n",
    "important to use the same Python version to load the files that was used to run the Pydra\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:09.157332Z",
     "iopub.status.busy": "2025-02-06T10:49:09.157127Z",
     "iopub.status.idle": "2025-02-06T10:49:09.206331Z",
     "shell.execute_reply": "2025-02-06T10:49:09.205842Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '<your-cache-root>/<task-cache-dir/_error.pklz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcloudpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<your-cache-root>/<task-cache-dir/_error.pklz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     error \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(error)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '<your-cache-root>/<task-cache-dir/_error.pklz'"
     ]
    }
   ],
   "source": [
    "import cloudpickle as cp\n",
    "\n",
    "with open(\"<your-cache-root>/<task-cache-dir/_error.pklz\", \"rb\") as f:\n",
    "    error = cp.load(f)\n",
    "\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing upstream issues\n",
    "\n",
    "Failures are common in scientific analysis, even for well tested workflows, due to\n",
    "the novel nature and of scientific experiments and known artefacts that can occur.\n",
    "Therefore, it is always to sanity-check results produced by workflows. When a problem\n",
    "occurs in a multi-stage workflow it can be difficult to identify at which stage the\n",
    "issue occurred.\n",
    "\n",
    "Currently in Pydra you need to step backwards through the tasks of the workflow, load\n",
    "the saved task object and inspect its inputs to find the preceding nodes. If any of the\n",
    "inputs that have been generated by previous nodes are not ok, then you should check the\n",
    "tasks that generated them in turn. For file-based inputs, you should be able to find\n",
    "the path of the preceding task's cache directory from the provided file path. However,\n",
    "for non-file inputs you may need to exhaustively iterate through all the task dirs\n",
    "in your cache root to find the issue.\n",
    "\n",
    "For example, in the following example workflow, if a divide by 0 occurs within the division\n",
    "node of the workflow, then an `float('inf')` will be returned, which will then propagate\n",
    "through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:09.208113Z",
     "iopub.status.busy": "2025-02-06T10:49:09.207911Z",
     "iopub.status.idle": "2025-02-06T10:49:09.672588Z",
     "shell.execute_reply": "2025-02-06T10:49:09.671931Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-13' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-14' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-15' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-16' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-17' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Add'>: attribute lookup Add on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-19' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.SafeDivide'>: attribute lookup SafeDivide on types failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-20' coro=<ConcurrentFuturesWorker.run() done, defined at /opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:186> exception=PicklingError(\"Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\")>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/concurrent/futures/process.py\", line 210, in _sendback_result\n",
      "    result_queue.put(_ResultItem(work_id, result=result,\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 exception=exception, exit_pid=exit_pid))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/queues.py\", line 391, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py\", line 194, in run\n",
      "    return await self.loop.run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.pool, self.unpickle_and_run, task_pkl, rerun\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "_pickle.PicklingError: Can't pickle <class 'types.Subtract'>: attribute lookup Subtract on types failed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "StateIndex()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Submitter(worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m sub:\n\u001b[0;32m----> 8\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkflow completed successfully, results saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:194\u001b[0m, in \u001b[0;36mSubmitter.__call__\u001b[0;34m(self, task_def)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task\u001b[38;5;241m.\u001b[39mis_async:  \u001b[38;5;66;03m# Only workflow tasks can be async\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrerun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mrun(task, rerun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrerun)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/futures.py:199\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/workers.py:51\u001b[0m, in \u001b[0;36mWorker.run_async\u001b[0;34m(self, task, rerun)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[DefType]\u001b[39m\u001b[38;5;124m\"\u001b[39m, rerun: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mrun_async(rerun\u001b[38;5;241m=\u001b[39mrerun)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/core.py:423\u001b[0m, in \u001b[0;36mTask.run_async\u001b[0;34m(self, rerun)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudit\u001b[38;5;241m.\u001b[39mmonitor()\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39m_run_async(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 423\u001b[0m     result\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     etype, \u001b[38;5;28meval\u001b[39m, etr \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/specs.py:678\u001b[0m, in \u001b[0;36mWorkflowOutputs._from_task\u001b[0;34m(cls, task)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, lazy_field \u001b[38;5;129;01min\u001b[39;00m attrs_values(workflow\u001b[38;5;241m.\u001b[39moutputs)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m         val_out \u001b[38;5;241m=\u001b[39m \u001b[43mlazy_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m         output_wf[name] \u001b[38;5;241m=\u001b[39m val_out\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/lazy.py:133\u001b[0m, in \u001b[0;36mLazyOutField._get_value\u001b[0;34m(self, graph, state_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     state_index \u001b[38;5;241m=\u001b[39m StateIndex()\n\u001b[0;32m--> 133\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m _, split_depth \u001b[38;5;241m=\u001b[39m TypeParser\u001b[38;5;241m.\u001b[39mstrip_splits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_nested\u001b[39m(task: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask[DefType]\u001b[39m\u001b[38;5;124m\"\u001b[39m, depth: \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.13.1/x64/lib/python3.13/site-packages/pydra/engine/submitter.py:491\u001b[0m, in \u001b[0;36mNodeExecution.task\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks  \u001b[38;5;66;03m# Ensure tasks are loaded\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: StateIndex()"
     ]
    }
   ],
   "source": [
    "from pydra.engine.submitter import Submitter\n",
    "from pydra.tasks.testing import SafeDivisionWorkflow\n",
    "\n",
    "wf = SafeDivisionWorkflow(a=10, b=5).split(denominator=[3, 2 ,0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Submitter(worker=\"cf\") as sub:\n",
    "        result = sub(wf)\n",
    "    \n",
    "print(f\"Workflow completed successfully, results saved in: {result.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the task directory where the issue first surfaced, iterate through every task\n",
    "cache dir and check the results for `float(\"inf\")`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:49:09.674670Z",
     "iopub.status.busy": "2025-02-06T10:49:09.674470Z",
     "iopub.status.idle": "2025-02-06T10:49:09.691352Z",
     "shell.execute_reply": "2025-02-06T10:49:09.690885Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iter() returned non-iterator of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(task_cache_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_result.pklz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     result \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTask \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_cache_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m!r}\u001b[39;49;00m\u001b[38;5;124;43m produced an infinite value for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfield_name\u001b[49m\u001b[38;5;132;43;01m!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: iter() returned non-iterator of type 'list'"
     ]
    }
   ],
   "source": [
    "import cloudpickle as cp\n",
    "from pydra.utils import user_cache_dir\n",
    "\n",
    "run_cache = user_cache_dir / \"run-cache\"\n",
    "\n",
    "for task_cache_dir in run_cache.iterdir():\n",
    "    with open(task_cache_dir / \"_result.pklz\", \"rb\") as f:\n",
    "        result = cp.load(f)\n",
    "    for field_name in result.outputs:\n",
    "        if result.outputs[field_name] == float('inf'):\n",
    "            print(f\"Task {task_cache_dir.name!r} produced an infinite value for {field_name!r}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
